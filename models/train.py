import json
import argparse
import os

from torch.utils.data import DataLoader
from src.KoGPTDatasets import KoGPTDataset, split_text_into_chunks_with_metadata
from src.model_manager import ModelManager
from src.training_manager import Training_Manager

from datasets import load_dataset


def parse_args():
    """
    í•™ìŠµì„ ìœ„í•œ ëª…ë ¹ì¤„ íŒŒë¼ë¯¸í„° ì„¤ì •
    """
    parser = argparse.ArgumentParser(description='KoGPT2 Fine-Tuning')

    # parser.add_argument('--train_path', type=str, default='../datasets/cleaned_train_dataset.json',
    #                     help='í›ˆë ¨ ë°ì´í„°ì…‹ ê²½ë¡œ')
    # parser.add_argument('--val_path', type=str, default='../datasets/cleaned_valid_dataset.json',
    #                     help='ê²€ì¦ ë°ì´í„°ì…‹ ê²½ë¡œ')
    
    
    parser.add_argument('--model_save_path', type=str, default='./results/kogpt2-finetuned',
                        help='ëª¨ë¸ ì €ì¥ ê²½ë¡œ')
    parser.add_argument('--max_length', type=int, default=512,
                        help='ì‹œí€€ìŠ¤ ìµœëŒ€ ê¸¸ì´')
    parser.add_argument('--batch_size', type=int, default=8,
                        help='ë°°ì¹˜ ì‚¬ì´ì¦ˆ')
    parser.add_argument('--num_epochs', type=int, default=5,
                        help='ì—í¬í¬ ìˆ˜')
    parser.add_argument('--learning_rate', type=float, default=5e-5,
                        help='í•™ìŠµë¥ ')

    return parser.parse_args()


def main():
    args = parse_args()

    os.makedirs(args.model_save_path, exist_ok=True)
    print('âœ… ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„± ì„±ê³µ!')
    
    dataset = load_dataset(
        "UICHEOL-HWANG/fairy_dataset"
    )
    
    train_data=dataset['train']
    val_data=dataset['valid']

    model_manager = ModelManager(learning_rate=args.learning_rate, epochs=args.num_epochs)
    tokenizer = model_manager.tokenizer

    print("âœ… ë°ì´í„° ë¶„í•  ì¤‘...")
    train_chunks = split_text_into_chunks_with_metadata(train_data, tokenizer, max_length=args.max_length, overlap=50)
    val_chunks = split_text_into_chunks_with_metadata(val_data, tokenizer, max_length=args.max_length, overlap=50)

    train_dataset = KoGPTDataset(train_chunks, tokenizer=tokenizer, max_length=args.max_length)
    val_dataset = KoGPTDataset(val_chunks, tokenizer=tokenizer, max_length=args.max_length)

    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size)
    print("âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ. í›ˆë ¨ ì‹œì‘...")

    trainer = Training_Manager(model_manager.model, train_loader, val_loader)

    # ğŸš€ **í›ˆë ¨ ì‹œì‘**
    print("ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    train_losses = trainer.train(args.num_epochs)

    # âœ… **í›ˆë ¨ ì™„ë£Œ ë¡œê·¸**
    if train_losses:
        print(f"âœ… ëª¨ë“  ì—í¬í¬ ì™„ë£Œ! ì´ {len(train_losses)}ê°œì˜ ì—í¬í¬ê°€ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("âŒ `train_losses`ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. `Training_Manager.train()`ì„ í™•ì¸í•˜ì„¸ìš”.")

    # âœ… **ê²€ì¦ ì‹œì‘**
    print("ğŸ”„ ëª¨ë¸ ê²€ì¦ ì‹œì‘...")
    val_loss, val_accuracy = trainer.validation()

    print(f"âœ… í›ˆë ¨ ì™„ë£Œ! ìµœì¢… ê²€ì¦ ì†ì‹¤: {val_loss:.4f}, ì •í™•ë„: {val_accuracy:.4f}")

    # âœ… **ëª¨ë¸ ì €ì¥**
    model_manager.model.save_pretrained(args.model_save_path)
    model_manager.tokenizer.save_pretrained(args.model_save_path)
    print(f"âœ… ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ê°€ {args.model_save_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == '__main__':
    main()
