import argparse
import os

from torch.utils.data import DataLoader
from datasets import load_dataset

from src.KoGPTDatasets import KoGPTDataset, split_text_into_chunks_with_metadata
from src.model_manager import ModelManager
from src.training_manager import Training_Manager
from src.seperation_data import seperated_data


def parse_args():
    """
    í•™ìŠµì„ ìœ„í•œ ëª…ë ¹ì¤„ íŒŒë¼ë¯¸í„° ì„¤ì •
    """
    parser = argparse.ArgumentParser(description='KoGPT2 Fine-Tuning')

    parser.add_argument('--model_save_path', type=str, default='./results/kogpt2-finetuned',
                        help='ëª¨ë¸ ì €ì¥ ê²½ë¡œ')
    parser.add_argument('--max_length', type=int, default=512,
                        help='ì‹œí€€ìŠ¤ ìµœëŒ€ ê¸¸ì´')
    parser.add_argument('--batch_size', type=int, default=8,
                        help='ë°°ì¹˜ ì‚¬ì´ì¦ˆ')
    parser.add_argument('--num_epochs', type=int, default=5,
                        help='ì—í¬í¬ ìˆ˜')
    parser.add_argument('--learning_rate', type=float, default=5e-5,
                        help='í•™ìŠµë¥ ')

    return parser.parse_args()


def main():
    args = parse_args()

    # âœ… ëª¨ë¸ ì €ì¥ ë””ë ‰í„°ë¦¬ ìƒì„±
    os.makedirs(args.model_save_path, exist_ok=True)
    print('âœ… ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„± ì„±ê³µ!')

    # âœ… Hugging Face ë°ì´í„°ì…‹ ë¡œë“œ
    dataset = load_dataset("UICHEOL-HWANG/fairy_dataset")
    tmp_data = dataset['train']

    # âœ… ModelManager ì´ˆê¸°í™”
    model_manager = ModelManager(learning_rate=args.learning_rate, epochs=args.num_epochs)
    tokenizer = model_manager.tokenizer

    # âœ… ë°ì´í„° ë¶„í• 
    print("âœ… ë°ì´í„° ë¶„í•  ì¤‘...")
    train_data, val_data = seperated_data(tmp_data)
    train_chunks = split_text_into_chunks_with_metadata(train_data, tokenizer, max_length=args.max_length, overlap=50)
    val_chunks = split_text_into_chunks_with_metadata(val_data, tokenizer, max_length=args.max_length, overlap=50)

    train_dataset = KoGPTDataset(train_chunks, tokenizer=tokenizer, max_length=args.max_length)
    val_dataset = KoGPTDataset(val_chunks, tokenizer=tokenizer, max_length=args.max_length)

    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size)
    print("âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ. í›ˆë ¨ ì‹œì‘...")

    # âœ… Training Manager ì´ˆê¸°í™” ë° í•™ìŠµ
    trainer = Training_Manager(model_manager.model, train_loader, val_loader)

    print("ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    train_losses = trainer.train(args.num_epochs)

    if train_losses:
        print(f"âœ… ëª¨ë“  ì—í¬í¬ ì™„ë£Œ! ì´ {len(train_losses)}ê°œì˜ ì—í¬í¬ê°€ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("âŒ `train_losses`ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. `Training_Manager.train()`ì„ í™•ì¸í•˜ì„¸ìš”.")

    # âœ… ê²€ì¦
    print("ğŸ”„ ëª¨ë¸ ê²€ì¦ ì‹œì‘...")
    val_loss, val_accuracy = trainer.validation()
    print(f"âœ… í›ˆë ¨ ì™„ë£Œ! ìµœì¢… ê²€ì¦ ì†ì‹¤: {val_loss:.4f}, ì •í™•ë„: {val_accuracy:.4f}")

    # âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì €ì¥
    print("ğŸ’¾ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì €ì¥ ì¤‘...")
    model_manager.model.save_pretrained(args.model_save_path)
    model_manager.tokenizer.save_pretrained(args.model_save_path)
    print(f"âœ… ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ê°€ {args.model_save_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # âœ… Hugging Face í—ˆë¸Œì— ì—…ë¡œë“œ
    print("ğŸš€ Hugging Face í—ˆë¸Œì— ëª¨ë¸ ì—…ë¡œë“œ ì‹œì‘...")
    model_manager.model.push_to_hub("UICHEOL-HWANG/KoGPT2-fairytail")
    model_manager.tokenizer.push_to_hub("UICHEOL-HWANG/KoGPT2-fairytail")
    print("âœ… ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ê°€ Hugging Face í—ˆë¸Œì— ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == '__main__':
    main()
