from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel
from torch.optim import AdamW
from torch.nn import CrossEntropyLoss
import os


class ModelManager:
    """
    KoGPT2 ëª¨ë¸, í† í¬ë‚˜ì´ì €, ì˜µí‹°ë§ˆì´ì €, ì†ì‹¤ í•¨ìˆ˜ë¥¼ í†µí•© ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤
    """
    def __init__(self, learning_rate=5e-5, epochs=3):
        """
        Args:
            learning_rate (float): í•™ìŠµë¥ 
            epochs (int): í•™ìŠµ Epoch ìˆ˜
        """
        # âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
        self.model_path = 'skt/kogpt2-base-v2'
        
        if not os.path.exists(self.model_path):
            print(f"âš ï¸ ê²½ê³ : '{self.model_path}' ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. HuggingFace Hubì—ì„œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.")
        
        self.tokenizer = PreTrainedTokenizerFast.from_pretrained(
            self.model_path,
            bos_token='<s>',
            eos_token='</s>',
            pad_token='<pad>',
            unk_token='<unk>',
            mask_token='<mask>'
        )
        self.model = GPT2LMHeadModel.from_pretrained(self.model_path)
        
        # âœ… í•™ìŠµ ì„¤ì •
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.optimizer = AdamW(self.model.parameters(), lr=self.learning_rate)
        self.criterion = CrossEntropyLoss()
    
    def save(self, save_path: str):
        """
        ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œì»¬ ë””ë ‰í† ë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤.
        
        Args:
            save_path (str): ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        os.makedirs(save_path, exist_ok=True)
        self.model.save_pretrained(save_path)
        self.tokenizer.save_pretrained(save_path)
        print(f"âœ… ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ê°€ '{save_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def push_to_hub(self, repo_name: str):
        """
        ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ Hugging Face Hubì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.
        
        Args:
            repo_name (str): Hugging Face ë¦¬í¬ì§€í† ë¦¬ ì´ë¦„
        """
        self.model.push_to_hub(repo_name)
        self.tokenizer.push_to_hub(repo_name)
        print(f"ğŸš€ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ê°€ Hugging Face Hub '{repo_name}'ì— ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
